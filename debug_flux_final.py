#!/usr/bin/env python3
"""
Script de debug final FLUX avec mesure mÃ©moire et quantification optimisÃ©e

Tests:
1. GÃ©nÃ©ration standard avec diffÃ©rents dtypes (float32, float16 sur MPS)
2. GÃ©nÃ©ration quantifiÃ©e avec type compatible selon device
3. Mesure consommation mÃ©moire rÃ©elle
4. Nettoyage complet entre tests
"""

import os
import sys
import warnings
import torch
import datetime
import gc
import psutil
import time
from pathlib import Path

# Configuration environnement
os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
os.environ['TOKENIZERS_PARALLELISM'] = 'false'

# Supprimer warnings
warnings.filterwarnings("ignore", category=FutureWarning, module="timm.*")
warnings.filterwarnings("ignore", category=RuntimeWarning, message="invalid value encountered in cast")
warnings.filterwarnings("ignore", category=UserWarning, module="diffusers.*")

def get_memory_usage(device):
    """Mesure la consommation mÃ©moire selon le device"""
    try:
        if device == 'mps':
            # MÃ©moire GPU MPS
            gpu_memory = torch.mps.current_allocated_memory() / 1024**3  # GB
            return f"GPU: {gpu_memory:.2f} GB"
        elif device == 'cuda':
            # MÃ©moire GPU CUDA
            gpu_memory = torch.cuda.memory_allocated() / 1024**3  # GB
            return f"GPU: {gpu_memory:.2f} GB"
        else:
            # MÃ©moire RAM CPU
            ram_memory = psutil.Process().memory_info().rss / 1024**3  # GB
            return f"RAM: {ram_memory:.2f} GB"
    except Exception as e:
        return f"Erreur mesure: {e}"

def deep_cleanup(device):
    """Nettoyage mÃ©moire complet selon device"""
    try:
        # Force garbage collection
        gc.collect()
        
        if device == 'mps':
            torch.mps.empty_cache()
            torch.mps.synchronize()
        elif device == 'cuda':
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
        
        # Double garbage collection aprÃ¨s vidage cache
        gc.collect()
        
        print("   ğŸ§¹ Nettoyage mÃ©moire effectuÃ©")
        
    except Exception as e:
        print(f"   âš ï¸  Nettoyage partiel: {e}")

def check_environment():
    """VÃ©rifier l'environnement et les dÃ©pendances"""
    print("=" * 70)
    print("ğŸ” DIAGNOSTIC ENVIRONNEMENT COMPLET")
    print("=" * 70)
    
    # Device detection
    device = "mps" if torch.backends.mps.is_available() else ("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ¯ Device sÃ©lectionnÃ©: {device}")
    print(f"ğŸ“Š MÃ©moire initiale: {get_memory_usage(device)}")
    
    # Test imports
    try:
        from diffusers import FluxPipeline
        print("âœ… diffusers.FluxPipeline disponible")
    except ImportError as e:
        print(f"âŒ diffusers manquant: {e}")
        return False, None
    
    # Test quantification
    quantization_available = False
    try:
        from optimum.quanto import quantize, freeze, qfloat8, qint8, qint4
        # Test import int2 avec gestion d'erreur
        try:
            from optimum.quanto import qint2
            int2_available = True
            print("âœ… optimum.quanto disponible (avec qint2)")
        except ImportError:
            int2_available = False
            print("âœ… optimum.quanto disponible (sans qint2)")
        
        quantization_available = True
        
        # Configuration selon device
        if device == 'mps':
            print("ğŸ”§ Config MPS: qint8, qint4", "+ qint2" if int2_available else "")
        elif device == 'cuda':
            print("ğŸ”§ Config CUDA: qfloat8, qint8, qint4", "+ qint2" if int2_available else "")
        else:
            print("ğŸ”§ Config CPU: qint8, qint4", "+ qint2" if int2_available else "")
            
    except ImportError as e:
        print(f"âš ï¸  optimum.quanto manquant: {e}")
        int2_available = False
    
    print(f"ğŸ”§ Quantification: {'âœ… Disponible' if quantization_available else 'âŒ Non disponible'}")
    print()
    
    return True, {
        "device": device,
        "quantization_available": quantization_available,
        "int2_available": int2_available,
        "FluxPipeline": FluxPipeline
    }

def test_dtype_comparison(env_info):
    """Test diffÃ©rents dtypes sur MPS pour trouver le meilleur"""
    device = env_info["device"]
    FluxPipeline = env_info["FluxPipeline"]
    
    if device != 'mps':
        print("â­ï¸  Test dtype rÃ©servÃ© Ã  MPS, device actuel:", device)
        return None
    
    print("=" * 70)
    print("ğŸ§ª TEST DTYPE COMPARISON SUR MPS")
    print("=" * 70)
    
    results = {}
    dtypes_to_test = [
        (torch.float32, "float32", "sÃ»r, fix images noires"),
        (torch.float16, "float16", "half precision standard"),
        (torch.bfloat16, "bfloat16", "brain float, utilisÃ© avant dans le projet")
    ]
    
    for dtype, name, description in dtypes_to_test:
        print(f"\nğŸ”§ Test avec {name} ({description})...")
        
        try:
            # Mesure mÃ©moire avant
            memory_before = get_memory_usage(device)
            print(f"   ğŸ“Š MÃ©moire avant: {memory_before}")
            
            # Charger pipeline
            print("   ğŸ“¦ Chargement pipeline...")
            pipeline = FluxPipeline.from_pretrained(
                "black-forest-labs/FLUX.1-schnell",
                torch_dtype=dtype,
                use_safetensors=True
            )
            pipeline = pipeline.to(device)
            pipeline.enable_attention_slicing()
            
            # Mesure mÃ©moire aprÃ¨s chargement
            memory_loaded = get_memory_usage(device)
            print(f"   ğŸ“Š MÃ©moire chargÃ©: {memory_loaded}")
            
            # GÃ©nÃ©ration test
            generator = torch.Generator(device=device).manual_seed(42)
            
            with torch.inference_mode():
                result = pipeline(
                    prompt="a red apple on a white table",
                    width=512,
                    height=512,
                    num_inference_steps=4,
                    generator=generator,
                    guidance_scale=3.5
                )
                image = result.images[0]
            
            # Analyse qualitÃ©
            import numpy as np
            img_array = np.array(image)
            brightness = np.mean(img_array)
            
            # Sauvegarde
            timestamp = datetime.datetime.now().strftime('%H%M%S')
            filename = f"test_{name}_{timestamp}_bright_{brightness:.0f}.png"
            image.save(filename)
            
            # RÃ©sultats
            status = "âœ… OK" if brightness > 10 else "âŒ NOIR"
            results[name] = {
                "brightness": brightness,
                "status": status,
                "filename": filename,
                "memory_before": memory_before,
                "memory_loaded": memory_loaded
            }
            
            print(f"   ğŸ¨ RÃ©sultat: {status} (luminositÃ©: {brightness:.1f})")
            print(f"   ğŸ’¾ SauvÃ©: {filename}")
            
            # Nettoyage
            del pipeline, result, image
            deep_cleanup(device)
            
        except Exception as e:
            print(f"   âŒ Erreur {name}: {e}")
            results[name] = {"status": "âŒ ERREUR", "error": str(e)}
            deep_cleanup(device)
    
    # Comparaison finale
    print(f"\nğŸ“Š COMPARAISON DTYPES:")
    for dtype_name, result in results.items():
        if "brightness" in result:
            print(f"   {dtype_name}: {result['status']} - {result['brightness']:.1f}")
        else:
            print(f"   {dtype_name}: {result['status']}")
    
    return results

def test_standard_vs_quantized(env_info, best_dtype=torch.float32):
    """Test gÃ©nÃ©ration standard vs quantifiÃ©e avec mesure mÃ©moire"""
    device = env_info["device"]
    FluxPipeline = env_info["FluxPipeline"]
    
    print("=" * 70)
    print("ğŸ”¬ TEST STANDARD VS QUANTIFICATIONS MULTIPLES")
    print("=" * 70)
    
    results = {}
    
    # Configuration selon device - maintenant avec plusieurs options de quantification
    if device == 'mps':
        dtype = best_dtype
        # MPS supporte qint8, qint4, et potentiellement qint2
        quant_configs = [
            ("qint8", "8-bit integer"),
            ("qint4", "4-bit integer (ultra compression)")
        ]
        if env_info.get("int2_available", False):
            quant_configs.append(("qint2", "2-bit integer (extreme compression)"))
    elif device == 'cuda':
        dtype = torch.bfloat16
        # CUDA supporte tous les formats
        quant_configs = [
            ("qfloat8", "8-bit float (performance optimale)"),
            ("qint8", "8-bit integer"),
            ("qint4", "4-bit integer (ultra compression)")
        ]
        if env_info.get("int2_available", False):
            quant_configs.append(("qint2", "2-bit integer (extreme compression)"))
    else:
        dtype = torch.float32
        # CPU: formats conservateurs
        quant_configs = [
            ("qint8", "8-bit integer"),
            ("qint4", "4-bit integer")
        ]
        if env_info.get("int2_available", False):
            quant_configs.append(("qint2", "2-bit integer (extreme compression)"))
    
    print(f"ğŸ¯ Device: {device}, dtype={dtype}")
    print(f"ğŸ”¬ Tests de quantification: {len(quant_configs)} types")
    for name, desc in quant_configs:
        print(f"   - {name}: {desc}")
    
    # TEST 1: Standard
    print(f"\n1ï¸âƒ£ GÃ‰NÃ‰RATION STANDARD")
    try:
        time_total_start = time.time()
        
        memory_start = get_memory_usage(device)
        print(f"   ğŸ“Š MÃ©moire dÃ©part: {memory_start}")
        
        # Charger pipeline standard
        time_loading_start = time.time()
        pipeline = FluxPipeline.from_pretrained(
            "black-forest-labs/FLUX.1-schnell",
            torch_dtype=dtype,
            use_safetensors=True
        )
        pipeline = pipeline.to(device)
        pipeline.enable_attention_slicing()
        time_loading_end = time.time()
        
        memory_loaded = get_memory_usage(device)
        print(f"   ğŸ“Š MÃ©moire aprÃ¨s chargement: {memory_loaded}")
        print(f"   â±ï¸  Temps chargement: {time_loading_end - time_loading_start:.2f}s")
        
        # GÃ©nÃ©ration
        generator = torch.Generator(device=device).manual_seed(42)
        time_generation_start = time.time()
        with torch.inference_mode():
            result = pipeline(
                prompt="a red apple on a white table",
                width=512, height=512,
                num_inference_steps=4,
                generator=generator,
                guidance_scale=3.5
            )
            image = result.images[0]
        time_generation_end = time.time()
        
        memory_peak = get_memory_usage(device)
        print(f"   ğŸ“Š MÃ©moire pic gÃ©nÃ©ration: {memory_peak}")
        print(f"   â±ï¸  Temps gÃ©nÃ©ration: {time_generation_end - time_generation_start:.2f}s")
        
        # Analyse
        import numpy as np
        brightness = np.mean(np.array(image))
        
        time_total_end = time.time()
        
        timestamp = datetime.datetime.now().strftime('%H%M%S')
        filename = f"standard_{timestamp}_bright_{brightness:.0f}.png"
        image.save(filename)
        
        results['standard'] = {
            "brightness": brightness,
            "filename": filename,
            "memory_start": memory_start,
            "memory_loaded": memory_loaded,
            "memory_peak": memory_peak,
            "time_loading": time_loading_end - time_loading_start,
            "time_generation": time_generation_end - time_generation_start,
            "time_total": time_total_end - time_total_start,
            "status": "âœ… OK" if brightness > 10 else "âŒ NOIR"
        }
        
        print(f"   ğŸ¨ Standard: {results['standard']['status']} (luminositÃ©: {brightness:.1f})")
        print(f"   â±ï¸  Temps total: {results['standard']['time_total']:.2f}s")
        print(f"   ğŸ’¾ SauvÃ©: {filename}")
        
        # Nettoyage complet
        del pipeline, result, image
        deep_cleanup(device)
        
    except Exception as e:
        print(f"   âŒ Erreur standard: {e}")
        results['standard'] = {"status": "âŒ ERREUR", "error": str(e)}
        deep_cleanup(device)
    
    # TEST 2+: Quantifications multiples (si disponible)
    if env_info["quantization_available"]:
        from optimum.quanto import quantize, freeze, qfloat8, qint8, qint4
        # Import conditionnel pour qint2
        qint2 = None
        if env_info.get("int2_available", False):
            try:
                from optimum.quanto import qint2
            except ImportError:
                qint2 = None
        
        for i, (quant_name, quant_desc) in enumerate(quant_configs, 2):
            print(f"\n{i}ï¸âƒ£ GÃ‰NÃ‰RATION QUANTIFIÃ‰E ({quant_name})")
            print(f"   ğŸ“„ Description: {quant_desc}")
            
            try:
                # SÃ©lection du type de quantification
                if quant_name == "qfloat8":
                    quant_type = qfloat8
                elif quant_name == "qint8":
                    quant_type = qint8
                elif quant_name == "qint4":
                    quant_type = qint4
                elif quant_name == "qint2":
                    if qint2 is None:
                        print(f"   âŒ qint2 non disponible")
                        continue
                    quant_type = qint2
                else:
                    print(f"   âŒ Type quantification inconnu: {quant_name}")
                    continue
                
                time_total_start = time.time()
                
                memory_start = get_memory_usage(device)
                print(f"   ğŸ“Š MÃ©moire dÃ©part: {memory_start}")
                
                # Charger pipeline
                time_loading_start = time.time()
                pipeline = FluxPipeline.from_pretrained(
                    "black-forest-labs/FLUX.1-schnell",
                    torch_dtype=dtype,
                    use_safetensors=True
                )
                time_loading_end = time.time()
                
                memory_before_quant = get_memory_usage(device)
                print(f"   ğŸ“Š MÃ©moire avant quantification: {memory_before_quant}")
                print(f"   â±ï¸  Temps chargement: {time_loading_end - time_loading_start:.2f}s")
                
                # Quantification avec mesure de temps
                print(f"   ğŸ”§ Application quantification {quant_name}...")
                time_quantization_start = time.time()
                if hasattr(pipeline, 'transformer'):
                    quantize(pipeline.transformer, weights=quant_type)
                    freeze(pipeline.transformer)
                if hasattr(pipeline, 'text_encoder_2'):
                    quantize(pipeline.text_encoder_2, weights=quant_type)
                    freeze(pipeline.text_encoder_2)
                
                pipeline = pipeline.to(device)
                pipeline.enable_attention_slicing()
                time_quantization_end = time.time()
                
                memory_loaded = get_memory_usage(device)
                print(f"   ğŸ“Š MÃ©moire aprÃ¨s quantification: {memory_loaded}")
                print(f"   â±ï¸  Temps quantification: {time_quantization_end - time_quantization_start:.2f}s")
                
                # GÃ©nÃ©ration avec mesure de temps
                generator = torch.Generator(device=device).manual_seed(42)
                time_generation_start = time.time()
                with torch.inference_mode():
                    result = pipeline(
                        prompt="a red apple on a white table",
                        width=512, height=512,
                        num_inference_steps=4,
                        generator=generator,
                        guidance_scale=3.5
                    )
                    image = result.images[0]
                time_generation_end = time.time()
                
                memory_peak = get_memory_usage(device)
                print(f"   ğŸ“Š MÃ©moire pic gÃ©nÃ©ration: {memory_peak}")
                print(f"   â±ï¸  Temps gÃ©nÃ©ration: {time_generation_end - time_generation_start:.2f}s")
                
                # Analyse
                import numpy as np
                brightness = np.mean(np.array(image))
                
                time_total_end = time.time()
                
                timestamp = datetime.datetime.now().strftime('%H%M%S')
                filename = f"quantized_{quant_name}_{timestamp}_bright_{brightness:.0f}.png"
                image.save(filename)
                
                results[f'quantized_{quant_name}'] = {
                    "brightness": brightness,
                    "filename": filename,
                    "memory_start": memory_start,
                    "memory_before_quant": memory_before_quant,
                    "memory_loaded": memory_loaded,
                    "memory_peak": memory_peak,
                    "time_loading": time_loading_end - time_loading_start,
                    "time_quantization": time_quantization_end - time_quantization_start,
                    "time_generation": time_generation_end - time_generation_start,
                    "time_total": time_total_end - time_total_start,
                    "status": "âœ… OK" if brightness > 10 else "âŒ NOIR",
                    "quant_type": quant_name,
                    "quant_desc": quant_desc
                }
                
                print(f"   ğŸ¨ QuantifiÃ© {quant_name}: {results[f'quantized_{quant_name}']['status']} (luminositÃ©: {brightness:.1f})")
                print(f"   â±ï¸  Temps total: {results[f'quantized_{quant_name}']['time_total']:.2f}s")
                print(f"      â””â”€â”€ Chargement: {results[f'quantized_{quant_name}']['time_loading']:.2f}s")
                print(f"      â””â”€â”€ Quantification: {results[f'quantized_{quant_name}']['time_quantization']:.2f}s")
                print(f"      â””â”€â”€ GÃ©nÃ©ration: {results[f'quantized_{quant_name}']['time_generation']:.2f}s")
                print(f"   ğŸ’¾ SauvÃ©: {filename}")
                
                # Nettoyage complet
                del pipeline, result, image
                deep_cleanup(device)
                
            except Exception as e:
                print(f"   âŒ Erreur quantification {quant_name}: {e}")
                results[f'quantized_{quant_name}'] = {
                    "status": "âŒ ERREUR", 
                    "error": str(e),
                    "quant_type": quant_name
                }
                deep_cleanup(device)
    else:
        print(f"\n2ï¸âƒ£ QUANTIFICATION IGNORÃ‰E (optimum.quanto non disponible)")
    
    return results

def analyze_results(dtype_results, memory_results):
    """Analyse finale des rÃ©sultats"""
    print("=" * 70)
    print("ğŸ“Š ANALYSE FINALE DES RÃ‰SULTATS")
    print("=" * 70)
    
    # Dtype comparison
    if dtype_results:
        print("\nğŸ¯ MEILLEUR DTYPE SUR MPS:")
        for dtype_name, result in dtype_results.items():
            if "brightness" in result:
                print(f"   {dtype_name}: {result['status']} (luminositÃ©: {result['brightness']:.1f})")
    
    # Memory comparison
    if memory_results:
        print("\nğŸ’¾ CONSOMMATION MÃ‰MOIRE:")
        
        std_memory = None
        if 'standard' in memory_results and "memory_peak" in memory_results['standard']:
            std = memory_results['standard']
            print(f"   Standard: {std['memory_peak']}")
            try:
                std_memory = float(std['memory_peak'].split()[1])
            except:
                pass
        
        # Afficher toutes les quantifications testÃ©es
        quantized_results = [(k, v) for k, v in memory_results.items() if k.startswith('quantized_')]
        
        if quantized_results:
            print("\n   ğŸ”¬ Quantifications testÃ©es:")
            best_saving = 0
            best_quant = None
            
            for quant_key, quant_data in quantized_results:
                if "memory_peak" in quant_data and quant_data['status'] == "âœ… OK":
                    quant_type = quant_data['quant_type']
                    quant_desc = quant_data.get('quant_desc', '')
                    memory_peak = quant_data['memory_peak']
                    brightness = quant_data.get('brightness', 0)
                    time_total = quant_data.get('time_total', 0)
                    time_generation = quant_data.get('time_generation', 0)
                    time_quantization = quant_data.get('time_quantization', 0)
                    
                    print(f"      {quant_type}: {memory_peak} (luminositÃ©: {brightness:.1f})")
                    print(f"         â±ï¸  Temps total: {time_total:.2f}s | gÃ©nÃ©ration: {time_generation:.2f}s | quantification: {time_quantization:.2f}s")
                    
                    # Calcul Ã©conomie mÃ©moire
                    if std_memory:
                        try:
                            quant_mem = float(memory_peak.split()[1])
                            saving = ((std_memory - quant_mem) / std_memory) * 100
                            print(f"         ğŸ’¾ Ã‰conomie mÃ©moire: {saving:.1f}%")
                            
                            if saving > best_saving:
                                best_saving = saving
                                best_quant = quant_type
                        except:
                            print("         ğŸ’¾ Ã‰conomie mÃ©moire: calcul impossible")
                    
                    # Comparaison temps avec standard
                    if 'standard' in memory_results and 'time_generation' in memory_results['standard']:
                        std_time = memory_results['standard']['time_generation']
                        time_ratio = (time_generation / std_time) * 100
                        if time_ratio < 100:
                            print(f"         ğŸš€ {100-time_ratio:.1f}% plus rapide que standard")
                        elif time_ratio > 100:
                            print(f"         ğŸŒ {time_ratio-100:.1f}% plus lent que standard")
                        else:
                            print(f"         âš–ï¸  MÃªme vitesse que standard")
                            
                elif quant_key in memory_results:
                    quant_type = memory_results[quant_key].get('quant_type', quant_key)
                    status = memory_results[quant_key].get('status', 'Unknown')
                    print(f"      {quant_type}: {status}")
            
            if best_quant:
                print(f"\n   ğŸ† Meilleure quantification: {best_quant} ({best_saving:.1f}% Ã©conomie mÃ©moire)")
    
    # RÃ©sumÃ© temps de gÃ©nÃ©ration
    if memory_results:
        print("\nâ±ï¸  TEMPS DE GÃ‰NÃ‰RATION:")
        if 'standard' in memory_results and 'time_generation' in memory_results['standard']:
            std_time = memory_results['standard']['time_generation']
            print(f"   Standard: {std_time:.2f}s")
            
            # Trouver le plus rapide
            fastest_quant = None
            fastest_time = std_time
            for quant_key, quant_data in quantized_results:
                if quant_data.get('status') == "âœ… OK" and 'time_generation' in quant_data:
                    if quant_data['time_generation'] < fastest_time:
                        fastest_time = quant_data['time_generation']
                        fastest_quant = quant_data['quant_type']
            
            if fastest_quant:
                speedup = ((std_time - fastest_time) / std_time) * 100
                print(f"   ğŸƒ Plus rapide: {fastest_quant} ({fastest_time:.2f}s, +{speedup:.1f}% gain)")
    
    # Recommandations
    print("\nğŸ¯ RECOMMANDATIONS:")
    
    if dtype_results:
        working_dtypes = [name for name, result in dtype_results.items() 
                         if result.get('brightness', 0) > 10]
        if working_dtypes:
            print(f"   âœ… Dtypes fonctionnels: {', '.join(working_dtypes)}")
        else:
            print("   âŒ Aucun dtype ne fonctionne correctement")
    
    if memory_results:
        # Compter les quantifications qui fonctionnent
        working_quants = [k for k, v in memory_results.items() 
                         if k.startswith('quantized_') and v.get('status') == "âœ… OK"]
        
        if working_quants:
            print(f"   âœ… {len(working_quants)} quantification(s) fonctionnelle(s)")
            if best_quant:
                print(f"   ğŸ† Recommandation: {best_quant} (meilleure Ã©conomie mÃ©moire)")
        elif 'standard' in memory_results and memory_results['standard']['status'] == "âœ… OK":
            print("   âš ï¸  Utiliser mode standard uniquement (quantifications problÃ©matiques)")
        else:
            print("   âŒ ProblÃ¨mes dÃ©tectÃ©s, investigation nÃ©cessaire")

def main():
    """Fonction principale du test final optimisÃ©"""
    print("ğŸš€ DEBUG FLUX FINAL - OPTIMISÃ‰ AVEC MESURE MÃ‰MOIRE")
    print(f"ğŸ“… {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("ğŸ”§ Tests: dtype comparison + quantification + mesure mÃ©moire")
    print()
    
    # Diagnostic environnement
    env_ok, env_info = check_environment()
    if not env_ok:
        print("âŒ Environnement incompatible")
        return
    
    # Test dtype sur MPS
    dtype_results = test_dtype_comparison(env_info)
    
    # DÃ©termine le meilleur dtype
    best_dtype = torch.float32  # DÃ©faut sÃ»r
    if dtype_results:
        for dtype_name, result in dtype_results.items():
            if result.get('brightness', 0) > 10:
                if dtype_name == 'float16':
                    best_dtype = torch.float16  # PrÃ©fÃ¨re float16 si Ã§a marche
                    break
    
    # Test standard vs quantifiÃ© avec mesure mÃ©moire
    memory_results = test_standard_vs_quantized(env_info, best_dtype)
    
    # Analyse finale
    analyze_results(dtype_results, memory_results)
    
    print("\n" + "=" * 70)
    print("âœ… Tests terminÃ©s - VÃ©rifiez les images gÃ©nÃ©rÃ©es et mesures mÃ©moire")
    print("=" * 70)

if __name__ == "__main__":
    main()
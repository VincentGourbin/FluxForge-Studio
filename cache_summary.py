#!/usr/bin/env python3
"""
Complete cache summary for FluxForge Studio.
Shows current status, optimizations, and recommendations.
"""

import sys
from pathlib import Path

# Add src to path
src_path = Path(__file__).parent / "src"
sys.path.insert(0, str(src_path))

def main():
    from utils.model_cache import get_cache_stats
    import os
    
    print("üóÇÔ∏è FLUXFORGE STUDIO CACHE SUMMARY")
    print("=" * 60)
    
    # Current cache status
    stats = get_cache_stats()
    
    print("üìä CURRENT STATUS:")
    print(f"   Models cached: {stats['cached_models']}/{stats['total_models']} ({stats['cache_percentage']:.1f}%)")
    print(f"   Cache size: {stats['total_size_formatted']}")
    print()
    
    if stats['cache_percentage'] == 100.0:
        print("üéâ ALL REQUIRED MODELS CACHED!")
        print("‚úÖ Ready for offline use")
    else:
        print("‚ö†Ô∏è  Some models missing:")
        for model_name, info in stats["models"].items():
            if not info["cached"]:
                print(f"   ‚ùå {model_name}")
        print(f"\nüí° Run: python optimize_cache.py --predownload")
    
    print()
    
    # Check for optimization opportunities
    cache_dir = Path.home() / ".cache" / "huggingface" / "hub"
    obsolete_canny = cache_dir / "models--black-forest-labs--FLUX.1-Canny-dev"
    
    if obsolete_canny.exists():
        print("üíæ OPTIMIZATION OPPORTUNITY:")
        print("   Large standalone model can be replaced by LoRA version")
        print(f"   Potential savings: ~81GB")
        print(f"   Run: python show_cleanup_savings.py")
        print()
    
    # Show model breakdown
    print("üìã MODEL BREAKDOWN:")
    for model_name, info in stats["models"].items():
        status = "‚úÖ" if info["cached"] else "‚ùå"
        size_info = f"({info['size_formatted']})" if info["cached"] else ""
        
        # Add model type annotation
        if "lora" in model_name.lower():
            model_type = "[LoRA]"
        elif "depth-anything" in model_name.lower():
            model_type = "[Depth]"
        elif "rmbg" in model_name.lower():
            model_type = "[BG Removal]"
        else:
            model_type = "[Core]"
        
        print(f"   {status} {model_name} {model_type} {size_info}")
    
    print()
    print("üöÄ OPTIMIZATION BENEFITS:")
    print("   ‚Ä¢ LoRA models: Same quality, 98% less space")
    print("   ‚Ä¢ Automatic caching: No re-downloads")
    print("   ‚Ä¢ Offline capability: Works without internet")
    print("   ‚Ä¢ Faster loading: Local models load instantly")
    
    print("=" * 60)

if __name__ == "__main__":
    main()